{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shalespeare.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOj1WmKFdfSO7lzxMecAnWE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pjheslin/colab-notebooks/blob/main/Shalespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Auto-generate Shakespeare\n",
        "\n",
        "This is a crude charcter-based LSTM\n",
        "(Code adapted from Chollet, Chapter 8)\n",
        "\n",
        "See also Karpathy, [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
      ],
      "metadata": {
        "id": "DD6sNUecSdXI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM_ajmkGRt0D"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "print(keras.__version__)\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get Shakespeare's text from TensorFlow"
      ],
      "metadata": {
        "id": "DjH7ZS-cSnBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = tf.keras.utils.get_file('shakespeare.txt', \n",
        "                            'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
        "print('Corpus length:', len(text))"
      ],
      "metadata": {
        "id": "ZhbIjSmTR2TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will extract partially-overlapping sequences of length maxlen, one-hot encode them and pack them in a 3D Numpy array x of shape (sequences, maxlen, unique_characters). Simultaneously, we prepare a array y containing the corresponding targets: the one-hot encoded characters that come right after each extracted sequence."
      ],
      "metadata": {
        "id": "QylKqo6gSyou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of extracted character sequences\n",
        "maxlen = 60\n",
        "\n",
        "# We sample a new sequence every `step` characters\n",
        "step = 3\n",
        "\n",
        "# This holds our extracted sequences\n",
        "sentences = []\n",
        "\n",
        "# This holds the targets (the follow-up characters)\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('Number of sequences:', len(sentences))"
      ],
      "metadata": {
        "id": "IGSl6TUaSJ_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of unique characters in the corpus\n",
        "chars = sorted(list(set(text)))\n",
        "print('Unique characters:', len(chars))\n",
        "# Dictionary mapping unique characters to their index in `chars`\n",
        "char_indices = dict((char, chars.index(char)) for char in chars)"
      ],
      "metadata": {
        "id": "0vsRqYjmTUXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, one-hot encode the characters into binary arrays.\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1"
      ],
      "metadata": {
        "id": "kOclSCL9TiHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the network\n",
        "Our network is a single LSTM layer followed by a Dense classifier and softmax over all possible characters. "
      ],
      "metadata": {
        "id": "OwbOI4FkU0Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(layers.Dense(len(chars), activation='softmax'))"
      ],
      "metadata": {
        "id": "ErS4LsYmTteZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our targets are one-hot encoded, we will use categorical_crossentropy as the loss to train the model:"
      ],
      "metadata": {
        "id": "4gX58sNTVRRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
      ],
      "metadata": {
        "id": "M9uI3alAVKb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the language model and sampling from it\n",
        "\n",
        "\n",
        "Given a trained model and a seed text snippet, we generate new text by repeatedly:\n",
        "\n",
        " 1. Drawing from the model a probability distribution over the next character given the text available so far\n",
        " 2. Reweighting the distribution to a certain \"temperature\"\n",
        " 3. Sampling the next character at random according to the reweighted distribution\n",
        " 4. Adding the new character at the end of the available text\n",
        "\n",
        "This is the code we use to reweight the original probability distribution coming out of the model, and draw a character index from it (the \"sampling function\"):"
      ],
      "metadata": {
        "id": "bBm6sG9GVnti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "metadata": {
        "id": "U8EQ_ioNVUPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, this is the loop where we repeatedly train and generated text. We start generating text using a range of different temperatures after every epoch. This allows us to see how the generated text evolves as the model starts converging, as well as the impact of temperature in the sampling strategy."
      ],
      "metadata": {
        "id": "1XRXY5HlWEVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import sys\n",
        "\n",
        "for epoch in range(1, 60):\n",
        "    print('epoch', epoch)\n",
        "    # Fit the model for 1 epoch on the available training data\n",
        "    model.fit(x, y,\n",
        "              batch_size=128,\n",
        "              epochs=1)\n",
        "\n",
        "    # Select a text seed at random\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    generated_text = text[start_index: start_index + maxlen]\n",
        "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
        "\n",
        "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print('------ temperature:', temperature)\n",
        "        sys.stdout.write(generated_text)\n",
        "\n",
        "        # We generate 400 characters\n",
        "        for i in range(400):\n",
        "            sampled = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(generated_text):\n",
        "                sampled[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(sampled, verbose=0)[0]\n",
        "            next_index = sample(preds, temperature)\n",
        "            next_char = chars[next_index]\n",
        "\n",
        "            generated_text += next_char\n",
        "            generated_text = generated_text[1:]\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()"
      ],
      "metadata": {
        "id": "SfKhv7VJWAFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7EeA_i3hWKdL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}