{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPSP5gxSpTx2lM4bYcu7cHD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pjheslin/colab-notebooks/blob/main/gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text generation with GPT-2\n",
        "\n",
        "Code adapted from the [Huggingface blog](https://huggingface.co/blog/how-to-generate)."
      ],
      "metadata": {
        "id": "8bxts_ahyNk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "id": "mqYnITd3w-CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAF1mlq_eNxT"
      },
      "outputs": [],
      "source": [
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "pzPG_P5VwqGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "id": "UXiuRHVCeVXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer"
      ],
      "metadata": {
        "id": "sVPfzqjGw5kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "jouuXB9exePX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add the EOS token as PAD token to avoid warnings\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "Bu_TQyJOxgZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Greedy Search"
      ],
      "metadata": {
        "id": "KJa2BpxZyJd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode('To be or not to be, that is the', return_tensors='tf')"
      ],
      "metadata": {
        "id": "-9w6WxbvxnNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "greedy_output = model.generate(input_ids, max_length=100)"
      ],
      "metadata": {
        "id": "gW_lcFMwxxQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "pjUfKeusx0zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beam search\n",
        "We activate beam search and early_stopping"
      ],
      "metadata": {
        "id": "ysrmWlyxyoOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beam_output = model.generate(\n",
        "    input_ids, \n",
        "    max_length=100, \n",
        "    num_beams=5, \n",
        "    early_stopping=True\n",
        ")"
      ],
      "metadata": {
        "id": "JKOZhYu4x54G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "eLG8v9NDy-95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set no_repeat_ngram_size to 2"
      ],
      "metadata": {
        "id": "Z21WjbCmzIvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beam_output = model.generate(\n",
        "    input_ids, \n",
        "    max_length=100, \n",
        "    num_beams=5, \n",
        "    no_repeat_ngram_size=2, \n",
        "    early_stopping=True\n",
        ")\n",
        "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "ovoo7a7rzBpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sampling from the conditional probability distribution"
      ],
      "metadata": {
        "id": "nZEY7UuRzijJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed to reproduce results. \n",
        "tf.random.set_seed(2)\n",
        "\n",
        "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=100, \n",
        "    top_k=0\n",
        ")\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "GNvi56ApzL7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lowering the temperature decreases the likelihood of low probability events."
      ],
      "metadata": {
        "id": "Wl9krPtUz0Lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1)\n",
        "\n",
        "# use temperature to decrease the sensitivity to low probability candidates\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=100, \n",
        "    top_k=0, \n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "0qVd-PASznoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-K sampling"
      ],
      "metadata": {
        "id": "At_yWKnw0SOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1)\n",
        "\n",
        "# set top_k to 50\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=100, \n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "PjTqTajV0AUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-p (nucleus) sampling"
      ],
      "metadata": {
        "id": "3y6hkoeU2P1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1)\n",
        "\n",
        "# deactivate top_k sampling and sample only from 92% most likely words\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=100, \n",
        "    top_p=0.92, \n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "vjDiavDD0Zyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Afbt8vj52nC2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}